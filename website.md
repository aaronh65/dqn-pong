You need to have an introduction section, related work, method, etc. it is basically the same as a report but in the website format so that it is to navigate and share.

### Introduction
End-to-end learning algorithms for tasks like robotic manipulation and autonomous driving often consume raw pixels (in the case of cameras) as a representation of the current environment state. These systems hope to learn a nonlinear mapping from images to control in a way that optimizes some sort of objective. However, it is often hard to successfully train such a visuomotor model to convergence in a way that yields good performance, especially for complex tasks like autonomous navigation or \emph{Montezuma's Revenge}. One common hypothesis is that images are a very high-dimensional input, and it may be difficult to efficiently learn a mapping from state to control. For our project, we would like to investigate different methods of visual representation learning for planning and action in two domains: video games and simulated autonomous driving. We hope to incorporate auxiliary tasks when training the encoder and decoder to learn visual representations that better inform reinforcement learning policies in these domains.


### Related Work
Previous work has used multi-task setups to augment the performance of learned planners (ChauffeurNet, Uber NMP). One recent work has specifically tried to learn ``implicit affordances" - a compact learned representation - for reinforcement learning in autonomous driving. For our approach, the core aspect is learning an encoder $\phi_e$ that encodes a high-dimensional state $s$ input into a latent space. Here, we maintain the standard L2 reconstruction loss, but the novelty comes from the auxiliary tasks. In other words, one example loss function could be: $L = L2(\phi_d(\phi_e(s)), s) + L_\text{aux}$ where $\phi_d$ is a decoder.  We could, for instance, have one channel of the decoder predict the pixel locations of all monsters using ground truth segmentations to force the encoded latent space to maintain information about where monsters are (that should later be useful for planning). The specifics of the auxiliary tasks will likely differ depending on the testing environment. After training the encoder, we can try to train reinforcement/imitation learning approaches using encoded images instead of raw images, and compare performance on the appropriate benchmarks. 

### Overview of Approach
The classic ATARI games are a simple testbed for training and evaluating autonomous agents and are popular because they naturally provide interesting, dynamic environments (often with an explicit reward structure). We plan on working with three common games: Pong, Breakout, and Space Invaders. As a concrete example, let's consider ways in which we can learn visual representations for Space Invaders.

The OpenAI Space Invaders implementation has three main classes of actors: the player agent, the barriers, and the space invaders (which themselves come in multiple classes). The player agent and the space invaders will often shoot each other, which is visually represented as a thin white line. Our plan is to use the encoder-decoder setup mentioned previously where we ask the decoder to reconstruct the locations of each class of agent as the auxiliary task. For example, we could produce a segmentation mask of the bullets in the scene, and weight this reconstruction loss very high (since bullets are especially relevant to staying alive). There's no straightforward API for retrieving semantically segmented masks of the game window, but we can use basic heuristics to retrieve the masks (e.g. space invaders are color-coded by class). 